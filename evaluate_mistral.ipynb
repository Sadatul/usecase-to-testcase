{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will be removing the keys from the json objects, as they might impact the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_excel(f\"mistral_finetuned_output_{NUM_EPOCHS}epochs.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompts: input <br>\n",
    "Response: Test cases of reference dataset <br>\n",
    "Output: Test cases by fine tuned LLAMA 3.1 8B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.loc[10, \"Response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove \\n and \\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(value):\n",
    "\treturn value.strip().replace(\"\\n\", \" \").replace(\"\\t\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type in [\"Prompts\", \"Response\", \"Output\"]:\n",
    "\tresults[type] = results[type].apply(lambda x: remove_whitespace(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove fixed keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fixed = {} # remove all keys\n",
    "results_fixed[\"Response\"] = []\n",
    "results_fixed[\"Output\"] = []\n",
    "error_indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.loc[10, \"Output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "for idx in range(0, 200):\n",
    "\tboth_ok = True # check if both are json parseable\n",
    "\tfor type in [\"Response\", \"Output\"]:\n",
    "\t\ttcs = results.loc[idx, type]\n",
    "\t\ttry:\n",
    "\t\t\ttcs = json.loads(tcs)\n",
    "\t\texcept:\n",
    "\t\t\tboth_ok = False\n",
    "\t\t\tprint(type)\n",
    "\t\t\tprint(idx)\n",
    "\t\t\tprint(tcs)\n",
    "\t\n",
    "\tif both_ok:\n",
    "\t\tresults_fixed[\"Response\"].append(json.loads(results.loc[idx, \"Response\"]))\n",
    "\t\tresults_fixed[\"Output\"].append(json.loads(results.loc[idx, \"Output\"]))\n",
    "\telse:\n",
    "\t\terror_indices.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(results_fixed[\"Response\"]))\n",
    "print(len(results_fixed[\"Output\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_keys(d):\n",
    "    \"\"\"\n",
    "    remove all first level keys from d, convert it to a string\n",
    "    \"\"\"\n",
    "    return str(list(d.values()))[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type in [\"Response\", \"Output\"]:\n",
    "    for idx, tcs in enumerate(results_fixed[type]):\n",
    "        new_tcs = []\n",
    "        if not isinstance(tcs[\"testcases\"], list):\n",
    "            tcs[\"testcases\"] = [tcs[\"testcases\"]]\n",
    "        for tc in tcs[\"testcases\"]:\n",
    "            new_tcs.append(remove_keys(tc))\n",
    "        # break\n",
    "        results_fixed[type][idx] = \"; \".join(new_tcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the errored indices, use string replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_keys = ['\"testcases\":', '\"name\":', '\"description\":', '\"input\":', '\"expected\":']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_vals(s, vals):\n",
    "    for val in vals:\n",
    "        s = s.replace(val, \"\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in error_indices:\n",
    "\tresults_fixed[\"Response\"].append(remove_vals(results.loc[idx, \"Response\"], common_keys))\n",
    "\tresults_fixed[\"Output\"].append(remove_vals(results.loc[idx, \"Output\"], common_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(results_fixed[\"Response\"]))\n",
    "print(len(results_fixed[\"Output\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_fixed[\"Response\"][63])\n",
    "print(results_fixed[\"Output\"][63])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_fixed[\"Response\"][-1])\n",
    "print(results_fixed[\"Output\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate\n",
    "# !pip install absl-py\n",
    "# !pip install nltk\n",
    "# !pip install rouge-score\n",
    "# !pip install transformers\n",
    "# !pip install bert-score\n",
    "# !pip install --upgrade huggingface_hub\n",
    "# !pip install accelerate==0.31.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\t\n",
    "scores[\"rouge\"] = rouge.compute(predictions=results_fixed[\"Output\"], references=results_fixed[\"Response\"])\n",
    "\n",
    "print(f\"Finetuned scores: {scores[\"rouge\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\t\n",
    "scores[\"blue\"] = bleu.compute(predictions=results_fixed[\"Output\"], references=results_fixed[\"Response\"])\n",
    "\n",
    "print(f\"Finetuned scores: {scores[\"blue\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "bert = evaluate.load(\"bertscore\")\n",
    "\t\n",
    "scores[\"bert\"] = bert.compute(predictions=results_fixed[\"Output\"], references=results_fixed[\"Response\"], lang=\"en\")\n",
    "\n",
    "print(f\"Finetuned scores: {scores[\"bert\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"bert\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.mean(scores[\"bert\"][\"precision\"]))\n",
    "print(np.mean(scores[\"bert\"][\"recall\"]))\n",
    "print(np.mean(scores[\"bert\"][\"f1\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"bert\"][\"precision\"] = np.mean(scores[\"bert\"][\"precision\"])\n",
    "scores[\"bert\"][\"recall\"] = np.mean(scores[\"bert\"][\"recall\"])\n",
    "scores[\"bert\"][\"f1\"] = np.mean(scores[\"bert\"][\"f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f\"mistral_scores_final_fixed_finetuned_{NUM_EPOCHS}epochs.json\", \"w\") as fp:\n",
    "\tjson.dump(scores, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usecase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
