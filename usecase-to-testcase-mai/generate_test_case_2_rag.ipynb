{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Test case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import base64\n",
    "import getpass\n",
    "import json\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsonl import *\n",
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(ex_usecase, ex_testcases, usecase):\n",
    "    return \"\"\"You are a tester tasked with creating comprehensive test cases for a given usecase description.\n",
    "\n",
    "## Usecase description\n",
    "\"\"\" + ex_usecase + \"\"\"\n",
    "\n",
    "\n",
    "## Testcase \n",
    "\"\"\" + ex_testcases + \"\"\"\n",
    "\n",
    "\n",
    "## Usecase description\n",
    "\"\"\" + usecase + \"\"\"\n",
    "\n",
    "## Testcase\n",
    "\n",
    "\n",
    "--------\n",
    "**Important Instruction:**\n",
    "    - Understand the last usecase.\n",
    "    - Generate test cases similar to the given example that covers both:\n",
    "        - **Normal** and **Edge** case scenarios\n",
    "        - **Positive** and **Negative** case scenarios\n",
    "        - **Valid** and **Invalid** case scenarios\n",
    "    - Do not add any explanation or any unnecessary word.\n",
    "    - Your generated testcase must be json parsable and must follow the style of the given example.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": API_KEY,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_response(response: str) -> str:\n",
    "    if response is None:\n",
    "        return ''\n",
    "    \n",
    "    if \"```\" not in response:\n",
    "        return response\n",
    "\n",
    "    code_pattern = r'```((.|\\n)*?)```'\n",
    "    if \"```json\" in response:\n",
    "        code_pattern = r'```json((.|\\n)*?)```'\n",
    "\n",
    "    code_blocks = re.findall(code_pattern, response, re.DOTALL)\n",
    "\n",
    "    if type(code_blocks[-1]) == tuple or type(code_blocks[-1]) == list:\n",
    "        code_str = \"\\n\".join(code_blocks[-1])\n",
    "    elif type(code_blocks[-1]) == str:\n",
    "        code_str = code_blocks[-1]\n",
    "    else:\n",
    "        code_str = response\n",
    "\n",
    "    return code_str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_testcases(ex_usecase, ex_testcases, usecase):\n",
    "    # Payload for the request\n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": get_prompt(ex_usecase, ex_testcases, usecase)\n",
    "            },\n",
    "        ],\n",
    "        \"temperature\": 0.0,\n",
    "        \"top_p\": 0.95,\n",
    "        \"max_tokens\": 2000\n",
    "    }\n",
    "\n",
    "    ENDPOINT = \"https://qcri-llm-rag-3.openai.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2024-02-15-preview\"\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    response = requests.post(ENDPOINT, headers=headers, json=payload)\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    response.raise_for_status()\n",
    "    response = response.json()\n",
    "\n",
    "    cost = 0\n",
    "    cost += (2.5 * response[\"usage\"][\"prompt_tokens\"]) / 1e6\n",
    "    cost += (10 * response[\"usage\"][\"completion_tokens\"]) / 1e6\n",
    "\n",
    "    with open(\"stat.csv\", mode=\"a\") as file:\n",
    "        file.write(f'GPT4o,{response[\"usage\"][\"prompt_tokens\"]},{response[\"usage\"][\"completion_tokens\"]},{cost},{end_time-start_time}\\n')\n",
    "\n",
    "    # print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "    return json.loads(parse_response(response[\"choices\"][0][\"message\"][\"content\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bert_score(reference, candidate):\n",
    "    return 0, 0, 0\n",
    "    # P, R, F1 = score([candidate], [reference], lang=\"en\", verbose=False)\n",
    "    # return P.mean().item(), R.mean().item(), F1.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"dataset/dataset-20-rag.jsonl\"\n",
    "RESULTS_PATH = \"results/GPT4o-results-20-rag.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(RESULTS_PATH):\n",
    "    with open(RESULTS_PATH, mode=\"w\", encoding='utf-8') as file:\n",
    "        file.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = read_jsonl(RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_jsonl(DATASET_PATH)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, data in enumerate(dataset):\n",
    "    if len(results) > idx:\n",
    "        continue\n",
    "    \n",
    "    usecase = data[\"usecase\"]\n",
    "    ex_usecase = data[\"rag-example\"][\"usecase\"]\n",
    "    ex_testcases = data[\"rag-example\"][\"testcases\"]\n",
    "\n",
    "    if \"author\" in usecase: del usecase[\"author\"]\n",
    "    if \"id\" in usecase: del usecase[\"id\"]\n",
    "\n",
    "    usecase = json.dumps(usecase, indent=4)\n",
    "    ex_usecase = json.dumps(ex_usecase, indent=4)\n",
    "    ex_testcases = json.dumps(ex_testcases, indent=4)\n",
    "    \n",
    "    testcases = generate_testcases(ex_usecase, ex_testcases, usecase)\n",
    "\n",
    "    p, r, f1 = calculate_bert_score(\n",
    "        reference=json.dumps(data[\"testcases\"], indent=4),\n",
    "        candidate=json.dumps(testcases, indent=4),\n",
    "    )\n",
    "\n",
    "    results.append({\n",
    "        \"usecase\": data[\"usecase\"],\n",
    "        \"testcases\": data[\"testcases\"],\n",
    "        \"GPT4o_testcases\": testcases,\n",
    "        \"bert_score\": {\n",
    "            \"Precision\": p,\n",
    "            \"Recall\": r,\n",
    "            \"F1\": f1\n",
    "        }\n",
    "    })\n",
    "\n",
    "    write_jsonl(RESULTS_PATH, results)\n",
    "    # break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 93.40\n",
      "Average Recall: 93.75\n",
      "Average F1: 93.57\n"
     ]
    }
   ],
   "source": [
    "# precisions, recalls, f1_scores = [], [], []\n",
    "# for res in results:\n",
    "#     precisions.append(res[\"bert_score\"][\"Precision\"])\n",
    "#     recalls.append(res[\"bert_score\"][\"Recall\"])\n",
    "#     f1_scores.append(res[\"bert_score\"][\"F1\"])\n",
    "\n",
    "# print(f\"Average Precision: {(sum(precisions)*100)/len(precisions):0.2f}\")\n",
    "# print(f\"Average Recall: {(sum(recalls)*100)/len(recalls):0.2f}\")\n",
    "# print(f\"Average F1: {(sum(f1_scores)*100)/len(f1_scores):0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "u2t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
